---
title: 'Session 10: Data Science Capstone Project'
author: "Andreea Iordache"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: haddock
      theme: spacelab
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
div.navy1 { background-color:#686868; border-radius: 5px; padding: 20px; border-style: groove; color: #ffffff;}

</style>



```{r, load_libraries, include = FALSE}

if(!is.element("tidyverse", installed.packages()[,1]))
{  install.packages("tidyverse", repos = "http://cran.us.r-project.org")}

if(!is.element("Hmisc", installed.packages()[,1]))
{  install.packages("Hmisc", repos = "http://cran.us.r-project.org")} #package for data summary using `describe`

if(!is.element("ggplot2", installed.packages()[,1]))
{  install.packages("ggplot2", repos = "http://cran.us.r-project.org")} #package for plots
if(!is.element("ggthemes", installed.packages()[,1]))
{  install.packages("ggthemes", repos = "http://cran.us.r-project.org")} #package to make fancier ggplots

if(!is.element("janitor", installed.packages()[,1]))
{ install.packages("janitor", repos = "http://cran.us.r-project.org")} #package to visualize results of machine learning tools
if(!is.element("rpart.plot", installed.packages()[,1]))
{  install.packages("rpart.plot", repos = "http://cran.us.r-project.org")} #package to visualize trees

library(rpart.plot)
library(caret)
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate)
library(janitor) # clean_names()
library(Hmisc)
```

# Introduction and learning objectives

<div class = "navy1">
The purpose of this exercise is to build an estimation engine to guide investment decisions in London house market. You will first build machine learning algorithms (and tune them) to estimate the house prices given variety of information about each property. Then, using your algorithm, you will choose 200 houses to invest in out of about 2000 houses on the market at the moment.


<b>Learning objectives</b>

<ol type="i">
<li>Using different data mining algorithms for prediction.</li>
<li>Dealing with large data sets</li>
<li>Tuning data mining algorithms</li>
<li>Interpreting data mining algorithms and deducing importance of variables</li>
<li>Using results of data mining algorithms to make business decisions</li>
</ol>  
</div>

# Load data

There are two sets of data, i) training data that has the actual prices ii) out of sample data that has the asking prices. Load both data sets. 


```{r read-investigate}
#read in the data

london_house_prices_2019_training<-read.csv("training_data_assignment_with_prices.csv")
london_house_prices_2019_out_of_sample<-read.csv("test_data_assignment.csv")



#fix data types in both data sets

#fix dates
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate(date=as.Date(date))
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate(date=as.Date(date))
#change characters to factors
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate_if(is.character,as.factor)
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate_if(is.character,as.factor)

#take a quick look at what's in the data
str(london_house_prices_2019_training)
str(london_house_prices_2019_out_of_sample)

skimr::skim(london_house_prices_2019_training)

```
> There are 37 variables in the training datastet. The only variables that have missing data are address2, town and population (but for this is neglectable - less than 5% missing).

```{r split the price data to training and testing}
#let's do the initial split
library(rsample)

# Created a new variable for log price
london_house_prices_2019_training$log_price <- log(london_house_prices_2019_training$price)


train_test_split <- initial_split(london_house_prices_2019_training, prop = 0.75) #training set contains 75% of the data
# Create the training dataset
train_data <- training(train_test_split)
test_data <- testing(train_test_split)



```


# Visualize data 

Visualize and examine the data. What plots could be useful here? What do you learn from these visualizations?

```{r visualize}

ggplot(train_data, aes(x = price)) +
  geom_histogram(bins = 30, fill = "darkblue", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of Prices",
    x = "Asking Price (£)",
    y = ""
  ) +
  scale_x_continuous(labels = scales::comma)+
  theme_minimal()+
  theme(
        plot.title.position = "plot",
        panel.grid = element_blank()

  )

# Log-transforming the asking prices since the distribution is so right-skewed
# Create a histogram for the log-transformed asking prices
ggplot(train_data, aes(x = log_price)) +
  geom_histogram(bins = 30, fill = "darkblue", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of Log-Transformed Prices",
    x = "Log(Asking Price)",
    y = ""
  ) +
  theme_minimal()+
  theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )


```
> The distribution of the prices is heavily skewed to the right, indicating that the majority of  prices are clustered at lower values (under 2 million), while a small number of properties are priced significantly higher. The tail extends toward £9,000,000, suggesting the presence of outliers or luxury properties in the dataset.
> The log transformation of the asking prices has made the distribution much closer to a normal distribution,normalizing it, which is good for modeling. This means that the extreme values won't have as much impact in the analysis.

```{r, warning=FALSE }

library(ggplot2)
library(scales)

# Function to create and plot distributions for different variables
create_distribution_plot <- function(data, variable, bins = 30, fill_color = "darkblue", title_prefix = "Distribution of") {
  ggplot(data, aes(x = {{ variable }})) +
    geom_histogram(bins = bins, fill = fill_color, color = "black", alpha = 0.7) +
    labs(
      title = paste(title_prefix, deparse(substitute(variable))),
      x = deparse(substitute(variable)),
      y = ""
    ) +
    scale_x_continuous(labels = comma) +
    theme_minimal()+
    theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )
  
}

plot1 <- create_distribution_plot(train_data, average_income, bins = 30)
plot2 <- create_distribution_plot(train_data, distance_to_station, bins = 20)

plot1
plot2


```
> The average income seems to follow a normal distribution, slightly skewed to the right. This indicates that while most individuals have incomes clustered around a central range,there are some that have higher incomes.
The median income seems to be between $50,000 to $60,000.
Also, the data spans a range from approximately $30,000 to over $80,000, showing a moderate spread in income levels.

> The distance to station distribution is right-skewed, meaning that most properties are close to the station (less than 2 miles). 

```{r}
# Reorder property_type based on frequency
train_data <- train_data %>%
  mutate(property_type = fct_infreq(property_type))

ggplot(train_data, aes(x = property_type)) +
  geom_bar(fill = "darkblue", color = "black") +
  labs(title = "Frequency of Property Types", x = "Property Type", y = "") +
  theme_minimal() +
  theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )


# Bar Plot for 'whether_old_or_new'
ggplot(train_data, aes(x = whether_old_or_new)) +
  geom_bar(fill = "darkblue", color = "black") +
  labs(title = "Frequency of Old vs New Properties", x = "Old vs. New", y = "") +
  theme_minimal()+
   theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )
```
> Flats dominate the housing market, closely followed by terraced houses. Detached houses are the least sought-after (under 1000, compared to the 4000 flats).

> As for old vs. new properties, there seem to only be new builds.

```{r, warning=FALSE, message=FALSE}
ggplot(train_data, aes(x = distance_to_station, y = price)) +
  geom_point(color = "darkblue", alpha = 0.6) +
  labs(title = "Price vs Distance to Station",
       x = "Distance to Station (km)", y = "Price (£)") +
  scale_y_continuous(labels = scales::number)+
  theme_minimal()+
   theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )

# Scatter Plot: asking_price vs average_income
ggplot(train_data, aes(x = average_income, y = log(price))) +
  geom_point(color = "darkblue", alpha = 0.6) +
  labs(title = "Price vs Average Income",
       x = "Average Income (£)", y = "Price (£)") +
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  scale_y_continuous(labels = scales::number)+
  theme_minimal()+
   theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )

# Scatter Plot: asking_price vs num_tube_lines
ggplot(train_data, aes(x = num_tube_lines, y = price)) +
  geom_point(color = "darkblue", alpha = 0.6) +
  labs(title = "Price vs Number of Tube Lines",
       x = "Number of Tube Lines", y = "Price (£)") +
  scale_y_continuous(labels = scales::number)+
  theme_minimal()+
   theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )
```
> As mentioned before, most properties are close to stations. There seems to be a negative correlation between the price and the distance to station, as short distances are associated with high prices and the price lowers as distance grows.
> There is a positive relationship between average income and log(price), meaning higher average incomes are associated with higher property prices.
There is some variability in property prices within similar income ranges, which implies that other factors might influence the price, but the upward slope of the red line confirms the overall trend.
> Most properties are located in areas that have 0, 2, or 4 tube lines.
Although the overall price gap is still significant at every level, properties with access to more tube lines typically have a broader variety of values, including higher-priced outliers.
The absence of a clear upward trend indicates that, although tube access may have an impact on property value, there are other factors that influence the price and the number of tube lines may not be a powerful predictor on its own.

```{r boxplot numeric}
train_numeric <- london_house_prices_2019_training %>% 
  select(total_floor_area,number_habitable_rooms, co2_emissions_potential, energy_consumption_current,
         co2_emissions_potential, energy_consumption_current, energy_consumption_potential,
         population, altitude, london_zone, average_income,price, num_tube_lines, num_rail_lines, 
         num_light_rail_lines, distance_to_station)

# Box Plot: asking_price by property_type
ggplot(london_house_prices_2019_training, aes(x = property_type, y = price)) +
  geom_boxplot(fill = "white", color = "black", outlier.color = "red") +
  labs(title = "Price by Property Type",
       x = "", y = "Price (£)") +
  scale_y_continuous(labels = scales::number)+
  theme_minimal()+
   theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )

# Box Plot: asking_price by type_of_closest_station
ggplot(london_house_prices_2019_training, aes(x = type_of_closest_station, y = price)) +
  geom_boxplot(fill = "white", color = "black", outlier.color = "red") +
  labs(title = "Price by Type of Closest Station",
       x = "", y = "Price (£)") +
  scale_y_continuous(labels = scales::number)+
  theme_minimal()+
   theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )
```

> Outliers:
There are significant outliers in all categories of property_type. These high asking prices might be influencing the analysis disproportionately.
The majority of asking prices are concentrated near the lower end of the range, but the outliers skew 
Different property types (D, F, S, T) seem to have similar medians and interquartile ranges, suggesting no drastic differences in typical asking prices between categories.
The same can be said about the type of station.
> As with the other cases it would be best to log price.

```{r}
ggplot(london_house_prices_2019_training, aes(x = property_type, y = log(price))) +
  geom_boxplot(fill = "white", color = "black", outlier.color = "red") +
  labs(title = "Log of Price by Property Type",
       x = "Property Type", y = "Log of Price (£)") +
  theme_minimal()+
   theme(  
    plot.title.position = "plot",
    panel.grid = element_blank()
  )

```
> The log transformation has made the distribution more comparable, reducing the skewness.
All property types have outliers, but the spread of outliers is similar across property types.
The interquartile range (IQR) and medians are relatively consistent for "F," "S," and "T," which means the variability is similar for these categories.



```{r, correlation table, warning=FALSE, message=FALSE}

# produce a correlation table using GGally::ggcor()
# this takes a while to plot

library("GGally")
london_house_prices_2019_training %>% 
  select(-ID) %>% #keep Y variable last
  ggcorr(method = c("pairwise", "pearson"), layout.exp = 2,label_round=2, label = TRUE,label_size = 2,hjust = 1,nbreaks = 5,size = 2,angle = -20)

```

> The logged price is strongly correlated with total_floor_area (0.74) and number_habitable_rooms (0.72), meaning that as the total florr area and number of habitable rooms increases, so does the logged price.
There is a moderate positive correlation with average_income (0.42), which means that higher average income levels tend to have higher property prices.
These strong correlations suggest that these features are critical predictors for pricing models, so I will include them in my regression model.

>As for correlations between the independent variables:
total_floor_area is strongly correlated with number_habitable_rooms (0.82) and with co2_emissions_current (0.76).
The string correlations between total_floor_area and number_habitable_rooms suggest potential multicollinearity,so I will have to check VIF for my regression.

# Fit a linear regression model

> For my model I will use the variables that had the highest correlation with log_price:

```{r}
#Define control variables
control <- trainControl (
  method="cv",
  number=5,
  verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation
model2_lm<-train(
 log_price ~ total_floor_area + number_habitable_rooms + co2_emissions_current +
      average_income + london_zone+ longitude + water_company ,
  train_data,
  method = "lm",
  trControl = control
)

# summary of the results
summary(model2_lm)
```

> We can see a huge improvement for R-squared in this model, increasing from 18.7% to 73.3%, meaning that the predictors explain 73.3% of the variability of the logged price.
> Furthermore, all the coefficients except Leep water company are statistically significant at the 5% threshold (even at 0.1%!), having a p-value < 0.05.
> London zone has a negative impact on the price, meaning as the zone number increases, the price decreases by -1.27e-01 pounds. Longitude and Leep and SES water companies also have a negative effect on the price.
> All other variables have a positive impact, meaning that the price goes up for a unit increasse in the variables. For Thames water company the coefficient is interpreted as: if the water compny for the property is Thames, the price goes up by 0.0899319626 pounds compared to the base water company.

```{r, warning=FALSE}
library(car)
#Extract the final model from caret
final_lm <- model2_lm$finalModel

# Calculate VIF
vif_values <- vif(final_lm)

# Display VIF
vif_values



```
> Since VIF > 5, we don't have multicollinearity so we don't need to remove any variables.

## Predict the values in testing and out of sample data

> I will use Cook's distance to find and remove the most extreme outliers.

```{r}
cooksD <- cooks.distance(final_lm)
plot(cooksD, type = "h", main = "Cook's Distance")
influential <- which(cooksD > (4 / nrow(train_data)))  # Threshold for high influence
train_data_clean <- train_data[-influential, ]
# Transformed london_zone into a factor
#train_data_clean$london_zone <- as.factor(train_data_clean$london_zone)
model_lm <- lm(log_price ~  total_floor_area + number_habitable_rooms + co2_emissions_current +
      average_income + total_floor_area : london_zone + longitude + water_company , data = train_data_clean)

summary(model_lm)
```


```{r}
# Residuals vs. Fitted
plot(model_lm, which = 1)

```

> The residuals are roughly centered around zero but there is a slight curve, indicating potential non-linearity in the relationship.
There are 2 points that are outliers with unusually high residuals, which could influence the model, so I will remove them.

```{r}
# Extract fitted values
fitted_values <- fitted(model_lm)

# Identify points with fitted values greater than a threshold (e.g., 15.5)
high_fitted_indices <- which(fitted_values > 16)

#high_fitted_indices
#train_data_clean[high_fitted_indices, ]

```

```{r}
# Remove the points
train_data_clean <- train_data_clean[-high_fitted_indices, ]

# Refit the model
model_lm <- lm(log_price ~ total_floor_area + number_habitable_rooms + co2_emissions_current +
      average_income + total_floor_area : london_zone + longitude + water_company,
               data = train_data_clean)

```

```{r}
plot(fitted(model_lm), resid(model_lm), 
     main = "Residuals vs Fitted (Updated Model)", 
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

```
> Now we can see that the line is almost straight at 0 and there are no more extreme outliers.

```{r}
#qqnorm(residuals(model_lm), main = "Q-Q Plot of Residuals")
#qqline(residuals(model_lm), col = "red", lwd = 2)
```

>

```{r}
summary(model_lm)
```

> My final model has an R-squared of 77.48%, with all variables statistically significant. 
Below I use the predict function to test the performance of the model in testing data and summarize the performance of the linear regression model. How can you measure the quality of your predictions?

```{r}
# We can predict the testing values

lm_predictions <- predict(model_lm,test_data)

lr_results<-data.frame(  RMSE = RMSE(lm_predictions, test_data$price), 
                         Rsquare = R2(lm_predictions, test_data$price))


lr_results                         

#We can predict prices for out of sample data the same way
predictions_oos <- predict(model_lm,london_house_prices_2019_out_of_sample)
#predictions_oos
```

> I will compare my model to a random one to see if it performs better:

```{r}
baseline_rmse <- sqrt(mean((test_data$price - mean(train_data$price))^2))
baseline_rmse

```

> My model has a much higher RMSE than the random one, which actually means that it performs worse as far as predecition error goes.

> The R-squared of the training data is 0.77, while the one for testing is 0.63, which could mean that there is overfitting in the model.To address this I could apply regularization techniques such as Ridge or Lasso or to simplify the model by reducing the number of predictors.

> So, I am going to apply Ridge:

```{r}
library(glmnet)

# Prepare the data
# Ridge regression requires numeric matrices for predictors (X) and the response (y)
X <- model.matrix(log_price ~ total_floor_area + number_habitable_rooms + co2_emissions_current +
                   average_income + total_floor_area:london_zone + longitude + water_company, 
                   data = train_data_clean)[, -1] # Remove intercept column
y <- train_data_clean$log_price

# Set up cross-validation
control <- trainControl(
    method = "cv",
    number = 5,
    verboseIter = TRUE
)
set.seed(123)

# Train Ridge Regression with glmnet
# alpha = 0 indicates Ridge regression (Lasso would be alpha = 1)
ridge_model <- train(
    x = X,
    y = y,
    method = "glmnet",
    trControl = control,
    tuneGrid = expand.grid(
        alpha = 0, # Ridge regression
        lambda = seq(0.001, 0.1, length = 50) # Regularization parameter
    )
)

# Print the best model
ridge_model

# Plot cross-validation results for lambda
plot(ridge_model)

# Extract coefficients of the best model
best_lambda <- ridge_model$bestTune$lambda
coef(ridge_model$finalModel, s = best_lambda)

```

```{r}
test_X <- model.matrix(~ total_floor_area + number_habitable_rooms + co2_emissions_current +
                       average_income + total_floor_area:london_zone + longitude + water_company, 
                       data = test_data)[, -1]

# Using the best lambda found during training to make predictions
ridge_predictions_log <- predict(ridge_model$finalModel, newx = test_X, s = ridge_model$bestTune$lambda)

# Converting predicted log_price back to the original scale (price)
ridge_predictions <- exp(ridge_predictions_log)

# Evaluating RMSE and R-squared on the test data using original price
ridge_results <- data.frame(
    RMSE = RMSE(ridge_predictions, test_data$price),
    Rsquare = R2(ridge_predictions, test_data$price)
)

ridge_results

# Ensuring categorical variable levels match between training and out-of-sample data
london_house_prices_2019_out_of_sample$water_company <- factor(
    london_house_prices_2019_out_of_sample$water_company,
    levels = levels(train_data_clean$water_company)
)

# Creating the design matrix for out-of-sample data
oos_X <- model.matrix(~ total_floor_area + number_habitable_rooms + co2_emissions_current +
                      average_income + total_floor_area:london_zone + longitude + water_company, 
                      data = london_house_prices_2019_out_of_sample)[, -1]

# Verifying dimensions
ncol(oos_X) # Should match the number of predictors (10)

# Predicting log_price for out-of-sample data and converting to original scale
predictions_oos_log <- predict(ridge_model$finalModel, newx = oos_X, s = ridge_model$bestTune$lambda)
predictions_oos <- exp(predictions_oos_log)

#predictions_oos

```
> After applying Ridge we can see that the model has a much lower RMSE (311302.1	), now performing much better than the random model.

# Fit a tree model


```{r, warning=FALSE}
set.seed(123)

model2_tree <- train(
  price ~ total_floor_area  + distance_to_station+
               co2_emissions_current + average_income + london_zone+ num_tube_lines +
         water_company
    ,
  train_data,
  method = "rpart",
  trControl = control,
  tuneLength=10
)

#You can view how the tree performs
model2_tree$results

#You can view the final tree
rpart.plot(model2_tree$finalModel)

#you can also visualize the variable importance
importance <- varImp(model2_tree, scale=TRUE)
plot(importance)

```

> The total floor area variable has the highest importance, playing a significant role in predicting house prices. It is closely followed by london zone. Most of the water companies are redundant.

```{r}
# Predict on the test data
tree_predictions <- predict(model2_tree, test_data)
# Calculate RMSE and R²
tree_results <- data.frame(
  RMSE = RMSE(tree_predictions, test_data$price),
  Rsquare = R2(tree_predictions, test_data$price)
)

tree_results

```
> The RMSE is lower than for the Ridge regression (277687.4 vs 358559) and R-sqaured is 0.70 compared to 0.586, meaning that the tree model performs better.

## Hyperparameter tuning

```{r}

# Defining cross-validation control
control <- trainControl(method = "cv", number = 5)

# Custom grid of cp values
cp_grid <- expand.grid(cp = seq(0.001, 0.05, by = 0.005))
set.seed(123)

# Training the model
model2_tree <- train(
  price ~ total_floor_area + distance_to_station +
               co2_emissions_current + average_income + london_zone + num_tube_lines +
               water_company,
  data = train_data,
  method = "rpart",
  trControl = control,
  tuneGrid = cp_grid
)


model2_tree$results

# Visualizing the performance across cp values
plot(model2_tree)

# Viewing the final tree
rpart.plot(model2_tree$finalModel)

# Variable importance
importance <- varImp(model2_tree, scale = TRUE)
plot(importance, main = "Variable Importance (Tree Model)")

# Predicting on the test data
tree_predictions <- predict(model2_tree, test_data)
# Calculating RMSE and R²
tree_results <- data.frame(
  RMSE = RMSE(tree_predictions, test_data$price),
  Rsquare = R2(tree_predictions, test_data$price)
)

tree_results

```

```{r}
# Combine results for comparison
colnames(ridge_results) <- c("RMSE", "Rsquare")

comparison_results <- rbind(
  Linear_Regression = lr_results,
  Ridge_Regression = ridge_results,
  Decision_Tree = tree_results

)

comparison_results



```
> Here we can see clearly that the decision tree performs considerably better than the regression since it has the lowest RMSE and the highest R-squared. 
> One of the reasons for which Trees performs better in this case is that it capture non-linear relationships and interactions, while Ridge and Linear Regressions assume a linear relationship. For Ridge you usually need to explicitly include polynomial or interaction terms, which wasn't the case for my model (I only had one interaction term).
> Also, the decision tree automatically selects and splits on the most important variables, ignoring irrelevant or redundant features, while Ridge only penalizes the coefficients.
> Maybe the most important reason in my case is that decision trees are robust to outliers and to skewed distributions, while regression is sensitive.
> Also Important!!: Decision trees and the other following algorithms work well regardless of whether the target is log-transformed or not - this is not the case for regression.

# Other algorithms

> The first algorithm I will try is KNN:

# KNN

```{r}

tune_grid <- expand.grid(k = seq(3, 15, by = 2))
set.seed(123)

model_knn <- train(
  price ~ total_floor_area + number_habitable_rooms + co2_emissions_current +
    average_income + total_floor_area:london_zone + longitude + water_company,
  data = train_data,
  method = "knn",
  trControl = control,
  tuneGrid = tune_grid
)

```


```{r}
# Defining no resampling control
control_no_cv <- trainControl(method = "none")

# Specifing a single value for k in tuneGrid
tune_grid <- expand.grid(k = 3)  

# Train the k-NN model
set.seed(123)
model_knn <- train(
  price ~ total_floor_area + distance_to_station +
            co2_emissions_current + average_income + london_zone + num_tube_lines +
            water_company,
  data = train_data,
  method = "knn",
  trControl = control_no_cv,
  tuneGrid = tune_grid
)

model_knn

```

```{r}
# Predicting on the test dataset
knn_predictions <- predict(model_knn, test_data)

# Evaluating performance
knn_results <- data.frame(
  RMSE = RMSE(knn_predictions, test_data$price),
  Rsquare = R2(knn_predictions, test_data$price)
)

knn_results

```
> KNN performs worse than the Decision Tree and Ridge Regression (higher RMSE and lower Rsquare).

# Random Forest

```{r rf, warning=FALSE, message=FALSE}
library(randomForest)

# Define cross-validation control
#control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Specify the grid of hyperparameters to tune
tune_grid <- expand.grid(
  mtry = c(2, 3, 4, 5),          # Number of predictors randomly sampled at each split
  splitrule = "variance",         # Default for regression
  min.node.size = c(1, 5, 10)     # Minimum size of terminal nodes
)

# Training a Random Forest model with hyperparameter tuning
set.seed(123)  # For reproducibility
model_rf <- train(
  price ~ total_floor_area + number_habitable_rooms + co2_emissions_current +
    average_income + total_floor_area:london_zone + longitude + water_company,
  data = train_data,        # Training dataset
  method = "ranger",        # Random Forest with 'ranger' backend for efficiency
  trControl = control,      # Cross-validation control
  tuneGrid = tune_grid,     # Hyperparameter grid
  importance = "permutation" # Variable importance type
)


# View the results of tuning
#model_rf$results

# View the best tuned parameters
model_rf$bestTune

# Variable Importance Plot
importance <- varImp(model_rf, scale = TRUE)
plot(importance, main = "Variable Importance (Random Forest)")

```

## Predictions on test data

```{r rf-predictions}
# Predict on test data
rf_predictions <- predict(model_rf, test_data)

# Evaluate performance
rf_results <- data.frame(
  RMSE = RMSE(rf_predictions, test_data$price),
  Rsquare = R2(rf_predictions, test_data$price)
)

rf_results

```



# XGBoost

```{r xgb, warning=FALSE, message=FALSE}
library(xgboost)

# Defining cross-validation control
control_cv <- trainControl(
  method = "cv",    # Use cross-validation
  number = 3,       # 3-fold CV (reasonable for quick tuning)
  verboseIter = TRUE  # Show progress
)

# Defining a hyperparameter grid for tuning
tune_grid <- expand.grid(
  nrounds = c(50, 100),       # Test fewer boosting rounds
  max_depth = c(4, 6),        # Test shallower and deeper trees
  eta = c(0.1, 0.3),          # Test slower and faster learning rates
  gamma = c(0, 1),            # Test regularization
  colsample_bytree = c(0.8, 1), # Subsample ratio of columns
  min_child_weight = c(1, 3),  # Minimum sum of instance weight
  subsample = c(0.8, 1)        # Subsample ratio of training data
)

# Training the XGBoost model with hyperparameter tuning
set.seed(123)
model_xgb <- train(
  price ~ total_floor_area + number_habitable_rooms + co2_emissions_current +
    average_income + total_floor_area:london_zone + longitude + water_company,
  data = train_data,
  method = "xgbTree",  # XGBoost in caret
  trControl = control_cv,
  tuneGrid = tune_grid  # Hyperparameter grid
)

model_xgb

model_xgb$bestTune


```


```{r xgb-tuning}
# Retrain the model using the best parameters
final_tune_grid <- expand.grid(
  nrounds = model_xgb$bestTune$nrounds,
  max_depth = model_xgb$bestTune$max_depth,
  eta = model_xgb$bestTune$eta,
  gamma = model_xgb$bestTune$gamma,
  colsample_bytree = model_xgb$bestTune$colsample_bytree,
  min_child_weight = model_xgb$bestTune$min_child_weight,
  subsample = model_xgb$bestTune$subsample
)

set.seed(123)
final_model_xgb <- train(
  price ~ total_floor_area + distance_to_station +
            co2_emissions_current + average_income + london_zone + num_tube_lines +
            water_company,
  data = train_data,       # Full training dataset
  method = "xgbTree",
  trControl = trainControl(method = "none"),  # No resampling needed
  tuneGrid = final_tune_grid
)

# View the final model
final_model_xgb

```



```{r xgb-predictions}
# Predict on test data
xgb_predictions <- predict(final_model_xgb, test_data)

# Evaluate RMSE and R²
xgb_results <- data.frame(
  RMSE = RMSE(xgb_predictions, test_data$price),
  Rsquare = R2(xgb_predictions, test_data$price)
)

xgb_results

```


```{r}
# Predict on out-of-sample data
final_predictions_oos <- predict(final_model_xgb, london_house_prices_2019_out_of_sample)

# View predictions
head(final_predictions_oos)
```


# Stacking

Use stacking to ensemble your algorithms.

```{r}
meta_features <- data.frame(
  lm      = lm_predictions,
  ridge   = ridge_predictions,
  rpart   = tree_predictions,
  knn     = knn_predictions,
  rf      = rf_predictions,
  xgbTree = xgb_predictions
)
set.seed(123)
stack_model <- train(
  x = meta_features,             # Meta-features (predictors only)
  y = test_data$price,           # Target variable specified separately
  method = "lm",                 # Linear regression as meta-learner
  trControl = trainControl(method = "cv", number = 5)  # Cross-validation
)

stack_predictions <- predict(stack_model, newdata = meta_features)
comparison <- data.frame(
  Model = c("Linear Regression","Ridge Regression", "Decision Tree","KNN", "Random Forest", "XGBoost", "Stacked Model"),
  RMSE = c(
    RMSE(lm_predictions, test_data$price),
    RMSE(ridge_predictions, test_data$price),
    RMSE(tree_predictions, test_data$price),
    RMSE(knn_predictions, test_data$price),
    RMSE(rf_predictions, test_data$price),
    RMSE(xgb_predictions, test_data$price),
    RMSE(stack_predictions, test_data$price)
  ),
  Rsquare = c(
    R2(lm_predictions, test_data$price),
    R2(ridge_predictions, test_data$price),
    R2(knn_predictions, test_data$price),
    R2(tree_predictions, test_data$price),
    R2(rf_predictions, test_data$price),
    R2(xgb_predictions, test_data$price),
    R2(stack_predictions, test_data$price)
  )
)

options(scipen = 999)
comparison
```
> It is clear from this table that the stacked model performs the best, with a RMSE of 194758.7	 and an R-squared of 0.8541921.
> Besides it, the best model is XGB with the lowest RMSE of 202520.2 and a R-squared of 0.8433860. It is closely followed by Random Forest, which has a RMSE of 204179.8 and a R-squared of 0.8412225.
> The worst-performing model is the linear Regression with a RMSE of 774089.1 and a R-squared of 0.6290949.
> Ridge performs better than the Linear Regression, and the Decision trees performs better than Ridge, as previously mentioned.
> KNN only performs better than the Linear Regression, but is is surpassed by Ridge as model's variability.

# Pick investments

In this section you should use the best algorithm you identified to choose 200 properties from the out of sample data.

```{r,warning=FALSE,  message=FALSE }


numchoose=200

oos<-london_house_prices_2019_out_of_sample

#predict the value of houses
lm_preds    <- predict(model2_lm,  newdata = oos)
ridge_X <- model.matrix(~ total_floor_area + number_habitable_rooms + co2_emissions_current +
    average_income + total_floor_area:london_zone + longitude + water_company,
                        data = oos)[, -1]  # Remove intercept column

ridge_preds <- predict(ridge_model$finalModel, newx = ridge_X, s = ridge_model$bestTune$lambda, type = "response")

knn_preds    <- predict(model_knn,  newdata = oos)
rpart_preds <- predict(model2_tree,    newdata = oos)
rf_preds    <- predict( model_rf,       newdata = oos)
xgb_preds   <- predict(model_xgb,  newdata = oos)

meta_features <- data.frame(
  lm      = lm_preds,
  ridge   = ridge_preds,
  knn = knn_preds,
  rpart   = rpart_preds,
  rf      = rf_preds,
  xgbTree = xgb_preds
)

# Predict using the stacking model's meta-learner
oos$predict <- predict(stack_model, newdata = meta_features)

#Choose the ones you want to invest here
#Make sure you choose exactly 200 of them
oos$pred_profit <- (oos$predict - oos$asking_price) / oos$asking_price

# Identify the top 200 rows *by index* (no reordering of oos itself)
top_indices <- order(-oos$pred_profit)[1:numchoose]

#  'buy' column; set the top 200 to 1, the rest to 0
oos$buy <- 0
oos$buy[top_indices] <- 1
oos$predict <- NULL
oos$pred_profit <- NULL

#output your choices. Change the name of the file to your "lastname_firstname.csv"
write.csv(oos,"Iordache_Andreea.csv")

oos %>% 
  filter(buy == 1) %>% 
  summarise(n = n())
```
